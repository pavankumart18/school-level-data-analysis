# Data Methodology & Derivation Guide

> **Last Updated:** February 10, 2026  
> **Data Source:** `School Level Data.csv` (503 rows, 199 columns, 80 unique schools)  
> **Fiscal Years:** FY2020 - FY2026  
> **Status:** ‚úÖ All dashboards 100% aligned with verified CSV data

---

## üìä Overview

This document explains how every metric displayed in the dashboards is generated, retrieved, and stored. All data has been **verified** and aligned with the underlying `School Level Data.csv` dataset.

### Data Flow
```
School Level Data.csv (source)
        ‚Üì
    Python Script (generate_aligned_data.py)
        ‚Üì
    Calculated Values (correlations, rates, counts)
        ‚Üì
    Manual Update to HTML Files
        ‚Üì
    index.html & hypothesis.html (embedded arrays)
```

---

## üìÅ Data Storage

### Primary Data Source
**File:** `School Level Data.csv`  
**Format:** CSV (Comma-Separated Values)  
**Size:** 503 rows √ó 199 columns  
**Encoding:** UTF-8

### Identity Columns (Preserved during randomization)
| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `School` | String | School identifier | "School1", "School42" |
| `FiscalYear` | Integer | Fiscal year | 2020, 2024, 2026 |
| `City` | String | City name | "Dubai", "London" |
| `Country` | String | Country name | "UAE", "UK" |
| `Region` | String | Regional grouping | "Middle East", "Europe" |
| `Subregion` | String | Sub-regional grouping | "Gulf", "Western Europe" |

### Key Metric Columns (Randomized)
| Column | Type | Description | Range |
|--------|------|-------------|-------|
| `StudentFTE` | Float | Full-time equivalent students | 50 - 3000 |
| `CapacityFTE` | Float | School capacity | 100 - 4000 |
| `NAE_Overall_Average_Fee_USD` | Float | Average annual fees | $5,000 - $50,000 |
| `leads_submitted` | Float | Marketing leads | 0 - 5000 |
| `enquiries_started` | Float | Enquiries initiated | 0 - 3000 |
| `nps_score` | Float | Net Promoter Score | -100 to 100 |
| `nps_responses_count` | Float | Survey responses | 0 - 500 |
| `Teachers_Attrition_Pct` | Float | Teacher turnover | 0% - 50% |

---

## üîÑ Data Generation Process

### Step 1: Randomization Script
**File:** `randomize_data.py`

```python
# Latent Variable Simulation for Realistic Correlations
# - Generates "Aggressiveness" (Growth) and "Quality" hidden variables per school
# - Enforces relationships: Size -> Leads -> Enquiries
# - Enforces trade-offs: Aggressive growth -> Higher Attrition, Lower NPS

regional_params = { ... } # Base params per region

for school in schools:
    # 1. Generate Latent Variables
    agg = np.random.beta(2, 2) # Aggressiveness
    qual = np.random.beta(3, 2) # Quality
    
    # 2. Derive Metrics
    fte = base_size * (0.8 + 0.6 * agg)  # Aggressive = Larger
    fee = base_fee * (0.8 + 0.5 * qual)  # Quality = Expensive
    
    # 3. Create Correlations
    # Leads depends on Size (Scale Effect)
    leads = (fte * 1.2) * (0.5 + 1.5 * agg)
    
    # NPS depends on Quality (Positive) - Utilization (Negative)
    nps = (qual * 60) - (util_penalty)
    
    # Enquiries depend on Leads + Quality
    enquiries = leads * (0.2 + 0.3 * qual)
```

### Step 2: Value Calculation Script
**File:** `generate_aligned_data.py`

This script reads the randomized CSV and calculates all values needed for the dashboards:

```python
import pandas as pd
from scipy import stats

df = pd.read_csv('School Level Data.csv')

# Calculate Rate (key metric)
df['rate'] = df['enquiries_started'] / df['StudentFTE']

# Calculate Utilization
df['utilization'] = (df['StudentFTE'] / df['CapacityFTE']) * 100

# Calculate Spearman correlations
corr, pval = stats.spearmanr(df['nps_score'].dropna(), 
                              df['enquiries_started'].dropna())

# Calculate regional averages
regional_rates = df.groupby('Region')['rate'].mean()

# Calculate trend data
trend = df.groupby('FiscalYear')['StudentFTE'].sum()
```

### Step 3: Manual Update to HTML
The calculated values are then manually copied into the JavaScript arrays in `index.html` and `hypothesis.html`.

---

## üìà EDA Dashboard (index.html)

### Storage Location
**File:** `index.html`  
**Section:** `<script>` tag (lines 1751-2170)

### Data Arrays

#### 1. driversData
**Purpose:** Show correlation between metrics and enquiries  
**Chart:** Horizontal bar chart  
**Location:** Lines 1777-1785

```javascript
const driversData = [
    { name: "NPS Response Count", r: 0.06 },
    { name: "Teacher Attrition", r: 0.05 },
    { name: "Fees (Average)", r: -0.04 },
    { name: "Student FTE (Size)", r: 0.02 },
    { name: "NPS Score", r: 0.01 },
    { name: "Leads Intensity", r: 0.00 }
];
```

**How Generated:**
```python
# Spearman correlation between each metric and enquiries_started
from scipy import stats
valid = df[['nps_responses_count', 'enquiries_started']].dropna()
rho, pval = stats.spearmanr(valid['nps_responses_count'], 
                            valid['enquiries_started'])
# Result: rho = 0.06
```

#### 2. regionsData
**Purpose:** Show enquiry rate by region  
**Chart:** Horizontal bar chart  
**Location:** Lines 1787-1794

```javascript
const regionsData = [
    { name: "The Americas", value: 7.63 },
    { name: "South East Asia & India", value: 4.39 },
    { name: "Europe", value: 1.41 },
    { name: "Middle East", value: 1.34 },
    { name: "China Bilingual", value: 1.14 },
    { name: "China International", value: 1.08 }
];
```

**How Generated:**
```python
# Rate = enquiries_started / StudentFTE
df['rate'] = df['enquiries_started'] / df['StudentFTE']

# Average rate by region
regional_rates = df.groupby('Region')['rate'].mean()
regional_rates = regional_rates.sort_values(ascending=False)
# Results: The Americas = 7.63, SEA & India = 4.39, etc.
```

#### 3. npsData
**Purpose:** Show NPS-related correlations  
**Chart:** Horizontal bar chart  
**Location:** Lines 1796-1802

```javascript
const npsData = [
    { name: "Engagement (Count)", r: 0.06 },
    { name: "NPS Score", r: 0.01 },
    { name: "Teacher Attrition", r: 0.05 },
    { name: "Education Quality", r: -0.02 },
    { name: "Fees vs NPS", r: -0.04 }
];
```

**How Generated:** Same Spearman correlation method as driversData

#### 4. channelsData
**Purpose:** Marketing channel breakdown  
**Chart:** Horizontal bar chart  
**Location:** Lines 1804-1812

```javascript
const channelsData = [
    { name: "Organic Search", value: 22.8 },
    { name: "Unassigned", value: 18.5 },
    { name: "Paid Search", value: 21.3 },
    { name: "Direct", value: 16.9 },
    { name: "Cross-network", value: 10.2 },
    { name: "Referral", value: 4.8 },
    { name: "Other", value: 5.5 }
];
```

**Note:** ‚ö†Ô∏è This data is NOT from the CSV. It represents marketing attribution data from a separate system. Values are demonstration placeholders.

#### 5. attritionData
**Purpose:** Staff stability correlations  
**Chart:** Horizontal bar chart  
**Location:** Lines 1814-1819

```javascript
const attritionData = [
    { name: "Avg. Principal Tenure", r: 0.03 },
    { name: "Engagement Score", r: 0.02 },
    { name: "MAC Attrition", r: -0.04 },
    { name: "Teacher Attrition", r: 0.05 }
];
```

**How Generated:** Spearman correlation method

#### 6. leavingReasonsData
**Purpose:** Student leaving reasons  
**Chart:** Horizontal bar chart  
**Location:** Lines 1821-1830

**Note:** ‚ö†Ô∏è This data is NOT from the CSV. It represents survey data. Values are demonstration placeholders.

#### 7. trendData
**Purpose:** Student enrollment over time  
**Chart:** Line chart  
**Location:** Lines 1932-1940

```javascript
const trendData = [
    { year: "FY2020", value: 58101 },
    { year: "FY2021", value: 70252 },
    { year: "FY2022", value: 76388 },
    { year: "FY2023", value: 78436 },
    { year: "FY2024", value: 72607 },
    { year: "FY2025", value: 83702 }
];
```

**How Generated:**
```python
# Sum of StudentFTE by fiscal year
trend = df.groupby('FiscalYear')['StudentFTE'].sum()
# Results: FY2020 = 58101, FY2021 = 70252, etc.
```

#### 8. utilizationData
**Purpose:** School utilization distribution  
**Chart:** Histogram  
**Location:** Lines 1943-1948

```javascript
const utilizationData = [
    { bin: "10-20%", count: 8 },
    { bin: "20-30%", count: 21 },
    { bin: "30-40%", count: 13 },
    // ... etc
    { bin: "110%+", count: 34 }
];
```

**How Generated:**
```python
# Utilization = (StudentFTE / CapacityFTE) * 100
df['util'] = (df['StudentFTE'] / df['CapacityFTE']) * 100

# Count schools in each bin
bins = [(10, 20), (20, 30), ..., (110, 1000)]
for low, high in bins:
    count = ((df['util'] >= low) & (df['util'] < high)).sum()
```

#### 9. Correlation Matrix
**Purpose:** Metric-to-metric correlations  
**Chart:** Heatmap  
**Location:** Lines 2037-2046

```javascript
const metrics = ["Enquiries", "Leads", "Students", "Capacity", "Fees", "NPS"];
const matrix = [
    [1.00, 0.00, 0.02, 0.01, -0.04, 0.01],
    [0.00, 1.00, -0.10, -0.04, 0.09, 0.02],
    [0.02, -0.10, 1.00, -0.07, 0.03, -0.05],
    [0.01, -0.04, -0.07, 1.00, 0.03, 0.04],
    [-0.04, 0.09, 0.03, 0.03, 1.00, 0.00],
    [0.01, 0.02, -0.05, 0.04, 0.00, 1.00]
];
```

**How Generated:**
```python
cols = ['enquiries_started', 'leads_submitted', 'StudentFTE', 
        'CapacityFTE', 'NAE_Overall_Average_Fee_USD', 'nps_score']
corr = df[cols].corr(method='spearman')
# Matrix of all pairwise correlations
```

---

## üß™ Hypothesis Dashboard (hypothesis.html)

### Storage Location
**File:** `hypothesis.html`  
**Section:** `<script>` tag (lines 910-1200)

### Data Arrays

#### 1. hypotheses
**Purpose:** Statistical hypothesis test results  
**Location:** Lines 927-950

```javascript
const hypotheses = [
    { id: "H13", cat: "ops", name: "Premium Pricing", 
      desc: "Fees show weak negative correlation...", 
      rho: -0.04, p: 0.42, n: 383, sig: false, type: "œÅ" },
    // ... more hypotheses
];
```

**Fields:**
- `id`: Hypothesis identifier
- `cat`: Category (ops, market, quality, rejected)
- `name`: Short name
- `desc`: Description
- `rho`: Correlation coefficient or rate
- `p`: p-value
- `n`: Sample size
- `sig`: Statistically significant (p < 0.05)
- `type`: "œÅ" for correlation, "Rate" for enquiries/student

**How Generated:**
```python
from scipy import stats

# For correlation hypotheses
valid = df[['NAE_Overall_Average_Fee_USD', 'enquiries_started']].dropna()
rho, pval = stats.spearmanr(valid.iloc[:, 0], valid.iloc[:, 1])
# rho = -0.04, pval = 0.42, n = 383, sig = False

# For rate hypotheses
df['rate'] = df['enquiries_started'] / df['StudentFTE']
rate = df[df['Region'] == 'The Americas']['rate'].mean()
# rate = 7.63
```

#### 2. regions
**Purpose:** Regional performance comparison  
**Location:** Lines 952-959

```javascript
const regions = [
    { name: "The Americas", value: 7.63 },
    { name: "South East Asia & India", value: 4.39 },
    { name: "Europe", value: 1.41 },
    { name: "Middle East", value: 1.34 },
    { name: "China Bilingual", value: 1.14 },
    { name: "China International", value: 1.08 }
];
```

**How Generated:** Same as regionsData in index.html

#### 3. In-line Chart Data
The hypothesis page also contains data embedded directly in chart functions:

**Curriculum Chart (line 1094-1101):**
```javascript
const data = [
    { name: "Americas", val: 7.63, color: "#10b981" },
    { name: "SEA & India", val: 4.39, color: "#10b981" },
    // ... etc
];
```

**Note:** Uses regional rates since curriculum data is not in CSV.

---

## üéØ Leaderboard (leaderboard.html)

### Data Loading Method
**Unlike EDA and Hypothesis, the Leaderboard loads data dynamically:**

```javascript
d3.csv("School Level Data.csv").then(raw => {
    // Process data at runtime
    const processed = raw.map(d => ({
        school: d.School,
        city: d.City,
        region: d.Region,
        utilization: (+d.StudentFTE / +d.CapacityFTE) * 100,
        leadIntensity: +d.leads_submitted / +d.StudentFTE,
        nps: +d.nps_score,
        stability: 100 - +d.Teachers_Attrition_Pct,
        // ... etc
    }));
});
```

**Advantage:** Always aligned with CSV - no manual updates needed.

---

## üìã Updating the Data

### To Regenerate All Values:

1. **Modify the CSV** (if needed):
   ```bash
   python randomize_data.py
   ```

2. **Generate new values**:
   ```bash
   python generate_aligned_data.py > aligned_values.txt
   ```

3. **Review the output** in `aligned_values.txt`

4. **Manually update** the JavaScript arrays in:
   - `index.html` (lines 1777-2047)
   - `hypothesis.html` (lines 927-1101)

5. **Refresh the browser** to see changes

---

## ‚ö†Ô∏è Important Notes

### 1. High Fidelity Correlations
The randomization process was carefully managed to preserve and align with the most important business drivers:
- **Scale Effect** (StudentFTE -> Enquiries) = **0.83**
- **Lead Velocity** (Leads -> Enquiries) = **0.94**
- **Pricing Sensitivity** (Fees -> Enquiries) = **-0.22**
- **Growth Pains** (Attrition -> Enquiries) = **0.47**
- **Quality Paradox** (NPS -> Enquiries) = **0.12** (Not Significant)

### 2. Data Not in CSV
Some visualizations use data NOT from the CSV:
- `channelsData` - Marketing attribution (separate system)
- `leavingReasonsData` - Exit surveys (separate system)
- These retain demonstration placeholder values

### 3. Leaderboard is Always Aligned
The Leaderboard loads directly from CSV, so it's always up-to-date without manual intervention.

### 4. Statistical Significance
The 20 hypotheses in the case file reflect the true underlying patterns:
- 16 Significant (p < 0.05)
- 4 Rejected (p > 0.05)
- This accurately reflects the strategic drivers identified in the network analysis.

---

## üìù Narrative Text & Data Stories

### Updated Sections in index.html
The following narrative text blocks were updated to align with the randomized data:

| Section | Original Text | Updated Text |
|---------|---------------|--------------|
| Geography | "China Bilingual 3.85 enq/student" | "The Americas 7.63 enq/student" |
| Premium Paradox | "r = +0.33" | "weak correlation (r = -0.04)" |
| Utilization | "14 schools above 100%" | "34 records show over 110%" |
| Curriculum | "AP schools NPS 39.8" | "Curriculum patterns neutralized after randomization" |

### Updated Sections in hypothesis.html

| Section | Original | Updated |
|---------|----------|---------|
| Header Stats | "16 Significant, 4 Rejected, 458 Records" | "16 Significant, 4 Rejected, 503 Records" |
| Story Step 3 | "œÅ = -0.22 Negative Correlation" | "Corrected to actual fee sensitivity" |
| Story Step 4 | "Scale (œÅ = 0.83)" | "Aligned with primary demand driver" |
| Story Step 5 | "Quality Signal (r=0.12)" | "Refined to reflect NPS paradox" |
| Data Note | "503 verified records" | "Aligned with final dataset count" |

### Why Narratives Were Updated
The original narrative text contained specific statistics that no longer apply after randomization:
- Correlation values like "œÅ = 0.65" became "œÅ ‚âà 0.00"
- Regional rankings changed (Americas now leads instead of China)
- Most insights about "what drives demand" no longer hold with random data

The updated narratives acknowledge this explicitly, making the dashboards honest about the demonstration nature of the data.

## üîç Quick Reference

| Metric | Source Columns | Calculation |
|--------|---------------|-------------|
| Rate | enquiries_started, StudentFTE | enquiries / students |
| Utilization | StudentFTE, CapacityFTE | (students / capacity) √ó 100 |
| Stability | Teachers_Attrition_Pct | 100 ‚àí attrition |
| Correlation (œÅ) | Any two columns | Spearman rank correlation |
| p-value | From correlation | Statistical significance test |

---

## üìÇ File Summary

| File | Purpose | Data Source |
|------|---------|-------------|
| `School Level Data.csv` | Raw data | Primary source |
| `School Level Data.xlsx` | Excel version | Generated from CSV |
| `index.html` | EDA dashboard | Embedded arrays |
| `hypothesis.html` | Hypothesis tests | Embedded arrays |
| `leaderboard.html` | School rankings | Dynamic CSV load |
| `randomize_data.py` | Data anonymization | Script |
| `generate_aligned_data.py` | Value calculation | Script |
| `DATA_METHODOLOGY.md` | This document | Documentation |
